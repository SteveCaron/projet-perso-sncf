{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API SNCF: Les retards de train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Auteur :__ \n",
    "\n",
    "Steve Caron\n",
    "\n",
    "__Présentation :__ \n",
    "\n",
    "Ce script permet de requêter les enregistrements de toutes les arrivées de train dans les 200 gares les plus fréquentée de France. Il enregistre toutes les arrivées dans une table sql \"arrivees\" et il enregistre toutes les perturbations dans une table \"perturbations\".\n",
    "\n",
    "\n",
    "__Inputs :__ \n",
    "\n",
    "Un fichier ``data/top200gare.json`` contenant les informations permettant de faire les requêtes pour chaques gares. Les enregistrements sont classés en fonction de la fréquentation des gares.\n",
    "\n",
    "__Prerequis :__\n",
    "\n",
    "Un fichier nommé ````.env```` contenant les paramètres suivants\n",
    "\n",
    "    * API_KEY : clé d'acces à l'API SNCF\n",
    "    * DB_PASSWORD : mot de passe de l'utilisateur root de la base de données\n",
    "\n",
    "__Params :__\n",
    "\n",
    "* NB_GARE_TOP : n premières gares les plus fréquentées pour lesquelles on souhaite récuperer des informations\n",
    "* DB_PORT : Port de connexion à la base de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "from dataclasses import dataclass,asdict\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "from sqlalchemy.types import *\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_GARE_TOP = 200\n",
    "DB_PORT = 3306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_log_file_name = 'db.log'\n",
    "db_handler_log_level = logging.DEBUG\n",
    "db_logger_log_level = logging.DEBUG\n",
    "\n",
    "db_handler = logging.FileHandler(db_log_file_name)\n",
    "db_handler.setLevel(db_handler_log_level)\n",
    "\n",
    "db_logger = logging.getLogger('sqlalchemy')\n",
    "db_logger.addHandler(db_handler)\n",
    "db_logger.setLevel(db_logger_log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_en_string(dt):\n",
    "    '''Cette fonction convertit un datetime en chaîne de caractères'''\n",
    "    if str is None:\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.datetime.strftime(dt,'%Y%m%dT%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_en_datetime(str):\n",
    "    '''Cette fonction convertit une chaîne de caractères en datetime'''\n",
    "    if str is None:\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.datetime.strptime(str,\"%Y%m%dT%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(data,nom_fichier):\n",
    "    '''Cette fonction permet d'enregistrer un fichier JSON'''\n",
    "    with open(nom_fichier, \"w\") as fc:\n",
    "        json.dump(data, fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(data,nom_fichier):\n",
    "    '''Cette fonction permet d'enregistrer un fichier CSV'''\n",
    "    with open(nom_fichier,\"w\", newline='', encoding='utf-8')as fc:\n",
    "        writer = csv.DictWriter(fc,fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requete_api(code_gare,code_reseau,date,nb_gare):\n",
    "    '''Cette fonction effectue une requête API pour collecter la liste des arrivées pour une gare spécifique sur un réseau spécifique '''\n",
    "\n",
    "    base_url = \"https://api.sncf.com/v1/coverage/sncf\"\n",
    "    #Requete sans le filtre sur les trains\n",
    "    requete = f\"{base_url}/stop_areas/{code_gare}/networks/{code_reseau}/arrivals?from_datetime={date}&count={nb_gare}\"\n",
    "    reponse = requests.get(requete, auth=(api_key,\"\"))\n",
    "    reponse_json = reponse.json()\n",
    "    \n",
    "    return reponse_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification_dates(reponse_API,code_gare,code_reseau,date_requete,date_max:str):\n",
    "    '''Cette fonction permet de vérifier si toutes les arrivées comprises dans une réponse API sont avant la date max\n",
    "    Si c'est le cas la fonction retourne la réponse API initiale et la date de la dernière arrivée de la réponse initiale\n",
    "    Si ce n'est pas le cas, elle refait un appel API en faisant une requete comprennant uniquement les dates antérieurs à la date max\n",
    "    La fonction retourne alors la nouvelle réponse API et la date de la dernière arrivée comprise dans la réponse initiale'''\n",
    "    datetime_max = convertir_en_datetime(date_max)\n",
    "    arrivees_reponse=reponse_API[\"arrivals\"]\n",
    "    #Gestion du cas ou la requete ne revoie pas d'arrivée\n",
    "    if len(arrivees_reponse) == 0:\n",
    "        #On retourne des liste vide et un datetime qui sera forcement supérieur à la date max\n",
    "        return [],[],datetime.datetime(9999,1,1,12,12,00)\n",
    "    datetime_derniere_requete = convertir_en_datetime(arrivees_reponse[-1][\"stop_date_time\"][\"arrival_date_time\"])\n",
    "    if datetime_derniere_requete > datetime_max:\n",
    "        for compteur,arrivee in enumerate(arrivees_reponse):\n",
    "            arrivee_datetime = convertir_en_datetime(arrivee[\"stop_date_time\"][\"arrival_date_time\"])\n",
    "            if arrivee_datetime > datetime_max:\n",
    "                reponse = requete_api(code_gare,code_reseau,date_requete,compteur)\n",
    "                return reponse[\"arrivals\"],reponse[\"disruptions\"], datetime_derniere_requete\n",
    "    else:\n",
    "        return arrivees_reponse,reponse_API[\"disruptions\"], datetime_derniere_requete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requete_entre_dates(code_gare,code_reseau,date_min,date_max,liste_arrivee,liste_perturbation,compteur_requete):\n",
    "    '''Cette fonction permet de faire des requêtes pour récupérer des données sur tous les enregistrements de la journée.\n",
    "    Elle sépare en deux listes les informations concernant les départs et les informations concernant les arrivées'''\n",
    "\n",
    "    date_requete = date_min\n",
    "\n",
    "    while date_requete < date_max:\n",
    "        # Requete api\n",
    "        reponse_api = requete_api(code_gare,code_reseau,date_requete,10)\n",
    "        compteur_requete += 1\n",
    "        arrivees,perturbations,date_derniere_requete = verification_dates(reponse_api,code_gare,code_reseau,date_requete,date_max)\n",
    "        # Ajoute chaque arrivées de la requete à la liste\n",
    "        [liste_arrivee.append(arrivee) for arrivee in arrivees]\n",
    "        # Ajoute chaque perturbations de la requete à la liste\n",
    "        [liste_perturbation.append(perturbation)  for perturbation in perturbations]\n",
    "        \n",
    "        date_requete = convertir_en_string(date_derniere_requete + datetime.timedelta(seconds=1))\n",
    "        \n",
    "    return liste_arrivee,liste_perturbation,compteur_requete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_id(nom_fichier):\n",
    "    '''Cette fonction ouvre un fichier json et récupère une liste de toutes les clés d'un dictionnaire\n",
    "    Il retourne le fichier json dans une variable et la liste de toutes les clés'''\n",
    "\n",
    "    #Ouverture du fichier csv\n",
    "    with open(nom_fichier,\"r\") as jsonfil:\n",
    "        data_gare = json.load(jsonfil)\n",
    "    toutes_id = data_gare[\"id\"]\n",
    "    liste_cles = []\n",
    "    # J'ajoute toutes les clés du dictionnaire id dans une liste\n",
    "    [liste_cles.append(cle) for cle in toutes_id.keys()]\n",
    "    return data_gare,liste_cles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arrivee:\n",
    "    arrivee_id:                         str\n",
    "    gare_id:                            str | None\n",
    "    date_arrive:                        datetime.date | None\n",
    "    heure_arrivee_prevue:               datetime.time | None\n",
    "    heure_arrivee:                      datetime.time | None\n",
    "    retard:                             datetime.timedelta | None\n",
    "    network:                            str | None\n",
    "    ligne:                              str | None\n",
    "    trip:                               str | None\n",
    "    direction:                          str | None\n",
    "    disruption_id:                      str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Perturbation:\n",
    "    perturbation_id:                        str | None\n",
    "    debut:                                  datetime.datetime | None\n",
    "    fin:                                    datetime.datetime | None\n",
    "    effet:                                  str | None\n",
    "    message_display:                        str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_donnees(data,liste_cles:list):\n",
    "    '''Cette fonction permet d'extraire une donnée dans un dictionnaire'''\n",
    "    try:\n",
    "        for cle in liste_cles:\n",
    "            #Cas ou la donnée est stockée dans un dictionnaire\n",
    "            if type(data) is dict:\n",
    "                data = data[cle]\n",
    "            #Cas ou la données est stockée dans une liste, alors on prend le premier element de la liste\n",
    "            else:\n",
    "                data=data[0][cle]\n",
    "        return data\n",
    "    except (KeyError,IndexError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_retard(json_arrivee):\n",
    "    ''' Cette fonction calcul le retard entre l'horaire prévue et l'horaire d'arrivée, si le train est à l'heure ou en avance la fonction renvoie None'''\n",
    "    heure_prevue = convertir_en_datetime(extraire_donnees(json_arrivee,[\"stop_date_time\",\"base_arrival_date_time\"]))\n",
    "    heure_arrivee = convertir_en_datetime(extraire_donnees(json_arrivee,[\"stop_date_time\",\"arrival_date_time\"]))\n",
    "    try:\n",
    "        if heure_prevue < heure_arrivee:\n",
    "            #Le train est en retard\n",
    "            retard = heure_arrivee - heure_prevue\n",
    "            return retard\n",
    "        else:\n",
    "            #Le train est à l'heure ou en avance\n",
    "            return None\n",
    "    #Cas ou un des horaire est manquant\n",
    "    except TypeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_heure(datetime:datetime):\n",
    "    '''Transforme un objet datetime en objet time'''\n",
    "    try:\n",
    "        return datetime.time()\n",
    "    except AttributeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collecte_donnees_arrivee(json_arrivee):\n",
    "    '''Cette fonction instancie une arrivée et remplie les champs en extrayant les données d'un dictionnaire'''\n",
    "    arrivee = Arrivee(\n",
    "        arrivee_id = extraire_donnees(json_arrivee,[\"stop_point\",\"stop_area\",\"id\"]).split(\":\")[-1]+\"-\" \\\n",
    "            + convertir_en_datetime(extraire_donnees(json_arrivee,[\"stop_date_time\",\"arrival_date_time\"])).strftime(\"%Y%m%d\") + \"-\"\\\n",
    "            + extraire_donnees(json_arrivee,[\"display_informations\",\"trip_short_name\"]),\n",
    "        gare_id = extraire_donnees(json_arrivee,[\"stop_point\",\"stop_area\",\"id\"]),\n",
    "        date_arrive = convertir_en_datetime(extraire_donnees(json_arrivee,[\"stop_date_time\",\"arrival_date_time\"])).date(),\n",
    "        heure_arrivee_prevue = transformation_heure(convertir_en_datetime(extraire_donnees(json_arrivee,[\"stop_date_time\",\"base_arrival_date_time\"]))),\n",
    "        heure_arrivee = transformation_heure(convertir_en_datetime(extraire_donnees(json_arrivee,[\"stop_date_time\",\"arrival_date_time\"]))),\n",
    "        retard = calcul_retard(json_arrivee),\n",
    "        network = extraire_donnees(json_arrivee,[\"display_informations\",\"network\"]),\n",
    "        ligne = extraire_donnees(json_arrivee,[\"display_informations\",\"label\"]),\n",
    "        trip = extraire_donnees(json_arrivee,[\"display_informations\",\"trip_short_name\"]),\n",
    "        direction = extraire_donnees(json_arrivee,[\"display_informations\",\"direction\"]),\n",
    "        disruption_id = extraire_donnees(json_arrivee,[\"display_informations\",\"links\",\"id\"])\n",
    "    )\n",
    "    return asdict(arrivee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collecte_donnees_perturbation(json_perturbation):\n",
    "    '''Cette fonction instancie une perturbation et remplie les champs en extrayant les données d'un dictionnaire'''\n",
    "    perturbation = Perturbation(\n",
    "        perturbation_id = extraire_donnees(json_perturbation,[\"id\"]),\n",
    "        debut = convertir_en_datetime(extraire_donnees(json_perturbation,[\"application_periods\",\"begin\"])),\n",
    "        fin = convertir_en_datetime(extraire_donnees(json_perturbation,[\"application_periods\",\"end\"])),\n",
    "        effet = extraire_donnees(json_perturbation,[\"severity\",\"effect\"]),\n",
    "        message_display = extraire_donnees(json_perturbation,[\"messages\",\"text\"])\n",
    "    )\n",
    "    return asdict(perturbation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stockage_en_bdd(nom_fichier:str):\n",
    "    '''Cette fonction place des données dans un dataframe et stocke le dataframe dans une base de donnée'''\n",
    "\n",
    "    try:\n",
    "        #Chargement des données\n",
    "        df = pd.read_csv(nom_fichier)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Pas de fichier {nom_fichier}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    #Dictionnaire contenant les schéma de données\n",
    "    schema_arrivee = {\n",
    "        \"arrivee_id\": String(255),\n",
    "        \"gare_id\": String(255),\n",
    "        \"date_arrive\": Date,\n",
    "        \"heure_arrivee_prevue\": Time,\n",
    "        \"heure_arrivee\": Time,\n",
    "        \"retard\": Time,\n",
    "        \"network\": String(255),\n",
    "        \"ligne\": String(255),\n",
    "        \"trip\": String(255),\n",
    "        \"direction\": String(255),\n",
    "        \"disruption_id\": String(255)}\n",
    "\n",
    "\n",
    "    #Connexion à la base de données\n",
    "    con_string = f\"mysql+pymysql://root:{db_password}@localhost:{DB_PORT}/APP_SNCF\"\n",
    "    engine = create_engine(con_string,echo=False)\n",
    "    \n",
    "    try:\n",
    "        #Ajout des données à la base de donnée\n",
    "        with engine.connect() as con:\n",
    "            df.to_sql(nom_table,con,if_exists=\"append\",index=False, dtype=schema_arrivee)\n",
    "    except:\n",
    "        con.rollback()\n",
    "        print(\"Fait un petit rollback\")\n",
    "        raise\n",
    "\n",
    "    print(f\"Enregistrement dans la BDD de {len(df.axes[0])} arrivees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stockage_en_bdd_perturbations(nom_fichier:str):\n",
    "    '''Cette fonction permet de charger un fichier csv dans une BDD Mysql en utilisant pandad\n",
    "    Elle insert les données ligne par ligne pour gerer les cas ou la donnée est déjà stocker dans la BDD'''\n",
    "\n",
    "    try:\n",
    "        #Chargement des données\n",
    "        df = pd.read_csv(nom_fichier)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Enregistrement dans la BDD de 0 perturbations\")\n",
    "        return None\n",
    "    \n",
    "    schema_perturbation = {\n",
    "        \"perturbation_id\" : String(255),\n",
    "\t    \"debut\" : DATETIME,\n",
    "\t    \"fin\" : DATETIME,\n",
    "\t    \"effet\" : String(255),\n",
    "\t    \"message\" : String(255) }\n",
    "    \n",
    "    #Connexion à la base de données\n",
    "    con_string = f\"mysql+pymysql://root:{db_password}@localhost:{DB_PORT}/APP_SNCF\"\n",
    "    engine = create_engine(con_string,echo=False)\n",
    "\n",
    "    #Ajout des données à la base de donnée\n",
    "\n",
    "    compteur_enregistrement = 0\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        # Enregistrement des données ligne par ligne\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                df.iloc[i:i+1].to_sql(\"perturbations\",con,if_exists=\"append\",index=False, dtype=schema_perturbation)\n",
    "                compteur_enregistrement += 1\n",
    "            except IntegrityError:\n",
    "                #La cle primaire est deja présente dans la BDD\n",
    "                pass\n",
    "            except:\n",
    "                #Les autres erreurs\n",
    "                con.rollback()\n",
    "                print(\"Fait un petit rollback\")\n",
    "                raise\n",
    "    \n",
    "    print(f\"Enregistrement dans la BDD de {compteur_enregistrement} perturbations\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_gare,cle,date_min,date_max):\n",
    "    '''Cette fontion lance les fonctions pour traiter les données d'une gare'''\n",
    "    code_gare = data_gare[\"id\"].get(cle)\n",
    "    nom_gare = data_gare[\"nom\"].get(cle)\n",
    "    print(f\"Debut des requetes pour la gare: {nom_gare}\")\n",
    "\n",
    "    compteur_requete = 0\n",
    "    liste_code_reseau = []\n",
    "    liste_arrivees = []\n",
    "    liste_perturbations = []\n",
    "\n",
    "    #Récupération des identifiant réseaux pour la gare en cours\n",
    "    [liste_code_reseau.append(reseau[\"id\"]) for reseau in data_gare[\"networks\"].get(cle)]\n",
    "\n",
    "    # Je traite réseau par réseau\n",
    "    for reseau in liste_code_reseau:\n",
    "        liste_arrivees, liste_perturbations,compteur_requete = requete_entre_dates(code_gare,reseau,date_min,date_max,liste_arrivees,liste_perturbations,compteur_requete)\n",
    "\n",
    "    #Sauvegarde données brutes\n",
    "    nom_fichier_arrivees = f\"data/arrivees/{code_gare}-{date_max}.json\".replace(\":\",\"_\")\n",
    "    nom_fichier_perturbations = f\"data/perturbations/{code_gare}-{date_max}.json\".replace(\":\",\"_\")\n",
    "    to_json(liste_arrivees,nom_fichier_arrivees)\n",
    "    to_json(liste_perturbations,nom_fichier_perturbations)\n",
    "    \n",
    "    #Nettoyage des données\n",
    "    liste_arrivees_clean=[]\n",
    "    for arrivee in liste_arrivees:\n",
    "        data_clean_arrivees = collecte_donnees_arrivee(arrivee)\n",
    "        liste_arrivees_clean.append(data_clean_arrivees)    \n",
    "    liste_perturbations_clean=[]\n",
    "    for perturbation in liste_perturbations:\n",
    "        data_clean_perturbation =collecte_donnees_perturbation(perturbation)\n",
    "        liste_perturbations_clean.append(data_clean_perturbation)\n",
    "    \n",
    "    #Sauvegarde des données propres\n",
    "    nom_ficher_arrivees_clean = f\"data/arrivees_propres/{code_gare}-{date_max}.csv\".replace(\":\",\"_\")\n",
    "    nom_ficher_perturbations_clean = f\"data/perturbations_propres/{code_gare}-{date_max}.csv\".replace(\":\",\"_\")\n",
    "    #Vérifie si les listes contiennent des données\n",
    "    if liste_arrivees_clean:\n",
    "        to_csv(liste_arrivees_clean,nom_ficher_arrivees_clean)\n",
    "    if liste_perturbations_clean:\n",
    "        to_csv(liste_perturbations_clean,nom_ficher_perturbations_clean)\n",
    "    #Enregistrement en BDD\n",
    "    stockage_en_bdd(nom_ficher_arrivees_clean)\n",
    "    stockage_en_bdd_perturbations(nom_ficher_perturbations_clean)\n",
    "    return compteur_requete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération de la clé API\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation des string date permettant de faire les requete API \n",
    "aujourdhui = datetime.date.today()\n",
    "hier_debut_journee = datetime.datetime(year=aujourdhui.year, month=aujourdhui.month, day=aujourdhui.day-1, hour=0, minute=0 ,second=0)\n",
    "hier_fin_journee = datetime.datetime(year=aujourdhui.year, month=aujourdhui.month, day=aujourdhui.day-1, hour=23, minute=59 ,second=59)\n",
    "date_min = convertir_en_string(hier_debut_journee)\n",
    "date_max = convertir_en_string(hier_fin_journee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des clés dictionnaire contenant les informations des gares\n",
    "data_gare,liste_cle = liste_id(\"data/top200gare.json\")\n",
    "# Exécution du run pour chaque gare\n",
    "compteur_total = 0\n",
    "for cle in liste_cle[:NB_GARE_TOP]:\n",
    "    compteur = run(data_gare,cle,date_min,date_max)\n",
    "    compteur_total += compteur\n",
    "    print(f\"{compteur} requêtes effectuées pour un total de {compteur_total} requêtes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
